{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nimport random\nimport pandas as pd\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2022-01-04T09:37:08.84826Z","iopub.execute_input":"2022-01-04T09:37:08.848746Z","iopub.status.idle":"2022-01-04T09:37:14.500128Z","shell.execute_reply.started":"2022-01-04T09:37:08.848684Z","shell.execute_reply":"2022-01-04T09:37:14.499271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, **load_files** function from the scikit-learn library is to import dataset.\n\n* **train_files, valid_files, test_files** - numpy arrays containing file paths to images\n* **train_targets, valid_targets, test_targets** - numpy arrays containing onehot-encoded classification labels\n* **dog_names** - list of string-valued dog breed names for translating labels","metadata":{}},{"cell_type":"code","source":"from sklearn.datasets import load_files       \nfrom keras.utils import np_utils\nimport numpy as np\nfrom glob import glob\n\n# define function to load train, test, and validation datasets\ndef load_dataset(path):\n    data = load_files(path)\n    dog_files = np.array(data['filenames'])\n    dog_targets = np_utils.to_categorical(np.array(data['target']), 133)\n    return dog_files, dog_targets\n\n# load train, test, and validation datasets\ntrain_files, train_targets = load_dataset(r'../input/dogs-dataset/dogImages/train')\nvalid_files, valid_targets = load_dataset(r'../input/dogs-dataset/dogImages/valid')\ntest_files, test_targets = load_dataset(r'../input/dogs-dataset/dogImages/test')\n\n# load list of dog names\ndog_names = [item[20:-1] for item in sorted(glob(r'../input/dogs-dataset/dogImages/train/*//'))]\n\n# print statistics about the dataset\nprint('There are %d total dog categories.' % len(dog_names))\nprint('There are %s total dog images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\nprint('There are %d training dog images.' % len(train_files))\nprint('There are %d validation dog images.' % len(valid_files))\nprint('There are %d test dog images.'% len(test_files))","metadata":{"id":"92bb87fa","outputId":"41eee3cf-3cc3-4050-81ae-633766b1404d","execution":{"iopub.status.busy":"2022-01-04T09:37:14.501674Z","iopub.execute_input":"2022-01-04T09:37:14.502715Z","iopub.status.idle":"2022-01-04T09:38:11.547521Z","shell.execute_reply.started":"2022-01-04T09:37:14.502636Z","shell.execute_reply":"2022-01-04T09:38:11.546238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below cell is to record the size of images as they have varying sizes. And some pictures have more than one dog.","metadata":{}},{"cell_type":"code","source":"## What are the dog image sizes. \n## They are all varying sizes, for some images there are more than one dog \nfrom matplotlib.pyplot import figure, imshow, axis\nfrom matplotlib.image import imread\n\ndef showImages(list_of_files, col=10, wSize=5, hSize=5, mypath='.'):\n    fig = figure(figsize=(wSize, hSize))\n    number_of_files = len(list_of_files)\n    row = 10\n    if (number_of_files % col != 0):\n        row += 1\n    for i in range(row+10):\n        a=fig.add_subplot(row, col, i + 1)\n        image = imread(list_of_files[i])\n        imshow(image)\n        axis('off')","metadata":{"id":"7774f1d0","execution":{"iopub.status.busy":"2022-01-04T09:38:11.548993Z","iopub.execute_input":"2022-01-04T09:38:11.549652Z","iopub.status.idle":"2022-01-04T09:38:11.556982Z","shell.execute_reply.started":"2022-01-04T09:38:11.549615Z","shell.execute_reply":"2022-01-04T09:38:11.555913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Displaying the images for which we need to test (1st parameter in the function), 2nd and 3rd parameters specify the width and height of the images. 4th parameter is to specify how many images we want to display in a column.","metadata":{}},{"cell_type":"code","source":"showImages(test_files, wSize=20, hSize=20, col=4)","metadata":{"id":"8d926144","execution":{"iopub.status.busy":"2022-01-04T09:38:11.559048Z","iopub.execute_input":"2022-01-04T09:38:11.559305Z","iopub.status.idle":"2022-01-04T09:38:13.292236Z","shell.execute_reply.started":"2022-01-04T09:38:11.559269Z","shell.execute_reply":"2022-01-04T09:38:13.291354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below function is to plot the breed distribution. ","metadata":{}},{"cell_type":"code","source":"## this function plot the breeds distribution \ndef plot_breed(df):\n    labels = []\n    for i in range(df.shape[0]):\n        labels.append(dog_names[np.argmax(df[i])])\n\n    df_labels = pd.DataFrame(np.array(labels), columns=[\"breed\"]).reset_index(drop=True)\n\n    fig, ax = plt.subplots(figsize=(10,30))\n    df_labels['breed'].value_counts().plot(ax=ax, kind='barh').invert_yaxis()\n    ax.set_title('Distribution of Dog breeds')","metadata":{"id":"698c5916","execution":{"iopub.status.busy":"2022-01-04T09:38:13.293363Z","iopub.execute_input":"2022-01-04T09:38:13.293599Z","iopub.status.idle":"2022-01-04T09:38:13.301166Z","shell.execute_reply.started":"2022-01-04T09:38:13.293573Z","shell.execute_reply":"2022-01-04T09:38:13.300366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## breed distribution in test data\nimport matplotlib.pyplot as plt\nplot_breed(test_targets)","metadata":{"id":"0b19100e","execution":{"iopub.status.busy":"2022-01-04T09:38:13.302452Z","iopub.execute_input":"2022-01-04T09:38:13.302671Z","iopub.status.idle":"2022-01-04T09:38:17.522189Z","shell.execute_reply.started":"2022-01-04T09:38:13.302643Z","shell.execute_reply":"2022-01-04T09:38:17.520548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## breed distribution in train data\nplot_breed(train_targets)","metadata":{"id":"2ec18058","execution":{"iopub.status.busy":"2022-01-04T09:38:17.523846Z","iopub.execute_input":"2022-01-04T09:38:17.524636Z","iopub.status.idle":"2022-01-04T09:38:21.656157Z","shell.execute_reply.started":"2022-01-04T09:38:17.524595Z","shell.execute_reply":"2022-01-04T09:38:21.6553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_img(img_path):\n    img = cv2.imread(img_path)\n    cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    imgplot = plt.imshow(cv_rgb)\n    return imgplot","metadata":{"id":"609663a8","execution":{"iopub.status.busy":"2022-01-04T09:38:21.657675Z","iopub.execute_input":"2022-01-04T09:38:21.657929Z","iopub.status.idle":"2022-01-04T09:38:21.663943Z","shell.execute_reply.started":"2022-01-04T09:38:21.657898Z","shell.execute_reply":"2022-01-04T09:38:21.663006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\n#### this function returns the shape of image, image itself and the  intensity distribution of an image\ndef img_hist(df_image, label):\n    img = cv2.imread(df_image)\n    color = ('b','g','r')\n    for i,col in enumerate(color):\n        histr = cv2.calcHist([img],[i],None,[256],[0,256])\n        plt.plot(histr,color = col)\n        plt.xlim([0,256])\n        \n    print(dog_names[np.argmax(label)])\n    print(img.shape)\n    plt.show()\n    #plt.imshow(img)\n    display_img(df_image)","metadata":{"id":"d0daaee3","execution":{"iopub.status.busy":"2022-01-04T09:38:21.665305Z","iopub.execute_input":"2022-01-04T09:38:21.665731Z","iopub.status.idle":"2022-01-04T09:38:21.924714Z","shell.execute_reply.started":"2022-01-04T09:38:21.665689Z","shell.execute_reply":"2022-01-04T09:38:21.923687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## here I checked the image sizes, and the  intensity distribution of images from the same breed\n## and the result shows images have different resolution, zoom and lightening conditins even for the same breed\n## it makes this task even more challenging \nimg_hist(train_files[3], train_targets[3])","metadata":{"id":"8a54aa5a","execution":{"iopub.status.busy":"2022-01-04T09:38:21.928866Z","iopub.execute_input":"2022-01-04T09:38:21.929204Z","iopub.status.idle":"2022-01-04T09:38:22.416015Z","shell.execute_reply.started":"2022-01-04T09:38:21.929167Z","shell.execute_reply":"2022-01-04T09:38:22.41503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_hist(train_files[4],train_targets[4])","metadata":{"id":"b28f665c","execution":{"iopub.status.busy":"2022-01-04T09:38:22.417556Z","iopub.execute_input":"2022-01-04T09:38:22.417812Z","iopub.status.idle":"2022-01-04T09:38:22.891231Z","shell.execute_reply.started":"2022-01-04T09:38:22.41778Z","shell.execute_reply":"2022-01-04T09:38:22.89033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_hist(train_files[57], train_targets[57])","metadata":{"id":"385b78fb","execution":{"iopub.status.busy":"2022-01-04T09:38:22.892588Z","iopub.execute_input":"2022-01-04T09:38:22.892837Z","iopub.status.idle":"2022-01-04T09:38:23.350632Z","shell.execute_reply.started":"2022-01-04T09:38:22.892806Z","shell.execute_reply":"2022-01-04T09:38:23.349801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below function is to find the list of labels and save them as a pandas data ","metadata":{}},{"cell_type":"code","source":"labels_train = []\nlabels_test = []\n\nfor i in range(train_files.shape[0]):\n    labels_train.append(dog_names[np.argmax(train_targets[i])])\n    \nfor i in range(test_files.shape[0]):\n    labels_test.append(dog_names[np.argmax(test_targets[i])])","metadata":{"id":"19a0dc97","execution":{"iopub.status.busy":"2022-01-04T09:38:23.35194Z","iopub.execute_input":"2022-01-04T09:38:23.352692Z","iopub.status.idle":"2022-01-04T09:38:23.38847Z","shell.execute_reply.started":"2022-01-04T09:38:23.352649Z","shell.execute_reply":"2022-01-04T09:38:23.387559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This function plots the breeds distribution in the train dataset","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\ndef dist_breed(labels):\n    encoder = LabelEncoder()\n    breeds_encoded = encoder.fit_transform(labels)\n    n_classes = len(encoder.classes_)\n    \n    breeds = pd.DataFrame(np.array(breeds_encoded), columns=[\"breed\"]).reset_index(drop=True)\n    breeds['freq'] = breeds.groupby('breed')['breed'].transform('count')\n    avg = breeds.freq.mean()\n    \n    title = 'Distribution of Dog Breeds in training Dataset\\n (%3.0f samples per class on average)' % avg\n    f, ax = plt.subplots(1, 1, figsize=(10, 6))\n    ax.set_xticks([])\n    \n    ax.hlines(avg, 0, n_classes - 1, color='white')\n    ax.set_title(title, fontsize=18)\n    _ = ax.hist(breeds_encoded, bins=n_classes)\n    \n    return(breeds[\"freq\"].describe())","metadata":{"id":"47058c1d","execution":{"iopub.status.busy":"2022-01-04T09:38:23.389812Z","iopub.execute_input":"2022-01-04T09:38:23.390043Z","iopub.status.idle":"2022-01-04T09:38:23.397572Z","shell.execute_reply.started":"2022-01-04T09:38:23.390014Z","shell.execute_reply":"2022-01-04T09:38:23.396881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dist_breed(labels_train)","metadata":{"id":"639eb127","execution":{"iopub.status.busy":"2022-01-04T09:38:23.398945Z","iopub.execute_input":"2022-01-04T09:38:23.399152Z","iopub.status.idle":"2022-01-04T09:38:23.889733Z","shell.execute_reply.started":"2022-01-04T09:38:23.399127Z","shell.execute_reply":"2022-01-04T09:38:23.888703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We use a pre-trained ResNet-50 model to detect dogs in images. The first line of code downloads the ResNet-50 model, along with weights that have been trained on ImageNet, a very large, very popular dataset used for image classification and other vision tasks. ImageNet contains over 10 million URLs, each linking to an image containing an object from one of 1000 categories. Given an image, this pre-trained ResNet-50 model returns a prediction (derived from the available categories in ImageNet) for the object that is contained in the image.","metadata":{}},{"cell_type":"code","source":"from tensorflow.python.keras.applications.resnet import ResNet50\n\n# define ResNet50 model\nResNet50_model = ResNet50(weights='imagenet')","metadata":{"id":"83af70f2","execution":{"iopub.status.busy":"2022-01-04T09:38:23.89129Z","iopub.execute_input":"2022-01-04T09:38:23.891627Z","iopub.status.idle":"2022-01-04T09:38:26.349248Z","shell.execute_reply.started":"2022-01-04T09:38:23.891584Z","shell.execute_reply":"2022-01-04T09:38:26.348343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Pre-process the Data**\nWhen using TensorFlow as backend, Keras CNNs require a 4D array (4D tensor) as input, with shape (nb_samples, rows, columns, channels), where nb_samples corresponds to the total number of images (or samples), and rows, columns, and channels correspond to the number of rows, columns, and channels for each image, respectively.\n\nThe path_to_tensor() function takes a string-valued file path to a color image as input and returns a 4D tensor suitable for supplying to a Keras CNN. The function first loads the image and resizes it to a square image that is 224 x 224 pixels. Next, the image is converted to an array, which is then resized to a 4D tensor. In this case, since we are working with color images, each image has three channels. Likewise, since we are processing a single image (or sample), the returned tensor will always have shape (1,224,224,3)\n\nThe paths_to_tensor function takes a numpy array of string-valued image paths as input and returns a 4D tensor with shape (nb_samples, 224, 224, 3)\n\nHere, nb_samples is the number of samples, or number of images, in the supplied array of image paths. We take nb_samples as the number of 3D tensors (where each 3D tensor corresponds to a different image) in this dataset.","metadata":{"id":"f9147305"}},{"cell_type":"code","source":"from keras.preprocessing import image                  \nfrom tqdm import tqdm\n\ndef path_to_tensor(img_path):\n    # loads RGB image as PIL.Image.Image type\n    img = image.load_img(img_path, target_size=(224, 224))\n    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n    x = image.img_to_array(img)\n    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n    return np.expand_dims(x, axis=0)\n\ndef paths_to_tensor(img_paths):\n    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n    return np.vstack(list_of_tensors)","metadata":{"id":"456f081a","execution":{"iopub.status.busy":"2022-01-04T09:38:26.350889Z","iopub.execute_input":"2022-01-04T09:38:26.351239Z","iopub.status.idle":"2022-01-04T09:38:26.360171Z","shell.execute_reply.started":"2022-01-04T09:38:26.351192Z","shell.execute_reply":"2022-01-04T09:38:26.3592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Making Predictions with ResNet-50**\nGetting the 4D tensor ready for ResNet-50, and for any other pre-trained model in Keras, requires some additional processing. First, the RGB image is converted to BGR by reordering the channels. All pre-trained models have the additional normalization step that the mean pixel (expressed in RGB as [103.939, 116.779, 123.68] and calculated from all pixels in all images in ImageNet) must be subtracted from every pixel in each image (implemented in the imported function preprocess_input)\n\nUsing the model to extract the predictions: Predict method, returns an array whose -th entry is the model's predicted probability that the image belongs to the -th ImageNet category (implemented in the ResNet50_predict_labels function)\n\nBy taking the argmax of the predicted probability vector, we obtain an integer corresponding to the model's predicted object class, which we can identify with an object category through the use of this [dictionary](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a).","metadata":{"id":"_fGd7JZ4g8GR"}},{"cell_type":"code","source":"from tensorflow.python.keras.applications.imagenet_utils import preprocess_input, decode_predictions\n\ndef ResNet50_predict_labels(img_path):\n    # returns prediction vector for image located at img_path\n    img = preprocess_input(path_to_tensor(img_path))\n    return np.argmax(ResNet50_model.predict(img))","metadata":{"id":"ea14db27","execution":{"iopub.status.busy":"2022-01-04T09:38:26.361555Z","iopub.execute_input":"2022-01-04T09:38:26.361857Z","iopub.status.idle":"2022-01-04T09:38:26.376946Z","shell.execute_reply.started":"2022-01-04T09:38:26.361814Z","shell.execute_reply":"2022-01-04T09:38:26.376221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Write a Dog Detector**\nIn the dictionary, the categories corresponding to dogs appear in an uninterrupted sequence and correspond to dictionary keys 151-268, inclusive, to include all categories from 'Chihuahua' to 'Mexican hairless'. Thus, in order to check to see if an image is predicted to contain a dog by the pre-trained ResNet-50 model, we need only check if the ResNet50_predict_labels function above returns a value between 151 and 268 (inclusive). \n\nImplemented using dog_detector() function, which returns True if a dog is detected in an image (and False if not)","metadata":{"id":"uh28hyT9zylA"}},{"cell_type":"code","source":"### returns \"True\" if a dog is detected in the image stored at img_path\ndef dog_detector(img_path):\n    prediction = ResNet50_predict_labels(img_path)\n    return ((prediction <= 268) & (prediction >= 151))","metadata":{"id":"ssbmkbctzuBS","execution":{"iopub.status.busy":"2022-01-04T09:38:26.378359Z","iopub.execute_input":"2022-01-04T09:38:26.378746Z","iopub.status.idle":"2022-01-04T09:38:26.390185Z","shell.execute_reply.started":"2022-01-04T09:38:26.378705Z","shell.execute_reply":"2022-01-04T09:38:26.389464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Create a CNN to Classify Dog Breeds**","metadata":{"id":"1qsI28fb329m"}},{"cell_type":"markdown","source":"### Pre-process the Data\nWe rescale the images by dividing every pixel in every image by 255.","metadata":{}},{"cell_type":"code","source":"from PIL import ImageFile                            \nImageFile.LOAD_TRUNCATED_IMAGES = True                 \n\n# pre-process the data for Keras\ntrain_tensors = paths_to_tensor(train_files).astype('float32')/255\nvalid_tensors = paths_to_tensor(valid_files).astype('float32')/255\ntest_tensors = paths_to_tensor(test_files).astype('float32')/255","metadata":{"id":"JKGYVTt337Ww","execution":{"iopub.status.busy":"2022-01-04T09:38:26.391566Z","iopub.execute_input":"2022-01-04T09:38:26.391779Z","iopub.status.idle":"2022-01-04T09:39:51.41294Z","shell.execute_reply.started":"2022-01-04T09:38:26.391752Z","shell.execute_reply":"2022-01-04T09:39:51.411883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Created a 4-layer CNN in Keras that classifies dog breeds with Relu activation function. The model starts with an input image of (224,224,3) color channels. The first layer produces an output with 16 feature channels that is used as an input for the next layer. The second, third, and last layer have 32, 64, 128 filters, respectively, with max-pooling of size 2. It would be ideal for input and output features to have the same size. So, I decided to use same padding, to go off the edge of images and pad with zeros, for all the layers in my network with the stride of 1. The number of nodes in the last fully connected layer is 133, the same size as dog categories, with softmax function to get the probabilities.","metadata":{"id":"zp9dmodi848S"}},{"cell_type":"code","source":"from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\nfrom keras.layers import Dropout, Flatten, Dense\nfrom keras.models import Sequential\n\nmodel = Sequential()\n\n# architucture\n\n# layer 1\nmodel.add(Conv2D(filters=16, kernel_size=2, padding='same', activation='relu', input_shape=(224,224,3)))\nmodel.add(MaxPooling2D(pool_size=2))\n\n# layer 2\nmodel.add(Conv2D(filters=32, kernel_size=2 , padding='same' , activation='relu'))\nmodel.add(MaxPooling2D(pool_size=2))\n\n# layer 3\nmodel.add(Conv2D(filters=64 , kernel_size=2 , padding='same' , activation='relu'))\nmodel.add(MaxPooling2D(pool_size=2))\nmodel.add(Dropout(0.4))\n\n# layer 4\nmodel.add(Conv2D(filters=128 , kernel_size=2 , padding='same' , activation='relu'))\nmodel.add(MaxPooling2D(pool_size=2))\nmodel.add(Dropout(0.4))\n\n# 2 fully connected layers\nmodel.add(Flatten())\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(133,activation='softmax'))\n\nmodel.summary()","metadata":{"id":"zu1w36GS87Zi","execution":{"iopub.status.busy":"2022-01-04T09:39:51.414768Z","iopub.execute_input":"2022-01-04T09:39:51.415186Z","iopub.status.idle":"2022-01-04T09:39:51.603348Z","shell.execute_reply.started":"2022-01-04T09:39:51.415142Z","shell.execute_reply":"2022-01-04T09:39:51.602489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# compile the model\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])","metadata":{"id":"_asGMIRi9Ug5","execution":{"iopub.status.busy":"2022-01-04T09:39:51.605035Z","iopub.execute_input":"2022-01-04T09:39:51.606063Z","iopub.status.idle":"2022-01-04T09:39:51.626897Z","shell.execute_reply.started":"2022-01-04T09:39:51.606002Z","shell.execute_reply":"2022-01-04T09:39:51.625751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using model checkpointing to save the model that attains the best validation loss.","metadata":{}},{"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint  \n\nepochs = 25\n\ncheckpointer = ModelCheckpoint(filepath='saved_models/weights.best.from_scratch.hdf5', \n                               verbose=1, save_best_only=True)\n\nhistory = model.fit(train_tensors, train_targets, \n          validation_data=(valid_tensors, valid_targets),\n          epochs=epochs, batch_size=20, callbacks=[checkpointer], verbose=1)","metadata":{"id":"hJD9c-4y9Uah","execution":{"iopub.status.busy":"2022-01-04T09:39:51.628575Z","iopub.execute_input":"2022-01-04T09:39:51.629097Z","iopub.status.idle":"2022-01-04T10:13:33.769837Z","shell.execute_reply.started":"2022-01-04T09:39:51.629061Z","shell.execute_reply":"2022-01-04T10:13:33.769185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the Model with the Best Validation Loss\nmodel.load_weights('saved_models/weights.best.from_scratch.hdf5')","metadata":{"id":"rFDa3Fjy-XgR","execution":{"iopub.status.busy":"2022-01-04T10:13:33.771344Z","iopub.execute_input":"2022-01-04T10:13:33.772072Z","iopub.status.idle":"2022-01-04T10:13:33.820675Z","shell.execute_reply.started":"2022-01-04T10:13:33.772036Z","shell.execute_reply":"2022-01-04T10:13:33.819861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Testing the model**","metadata":{"id":"FAuEHzrv_M2Q"}},{"cell_type":"markdown","source":"Trying out model on the test dataset of dog images. Aim to get test accuracy is greater than 1%","metadata":{}},{"cell_type":"code","source":"# get index of predicted dog breed for each image in test set\ndog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n\n# report test accuracy\ntest_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)","metadata":{"id":"iLYv-rhQ-ctz","execution":{"iopub.status.busy":"2022-01-04T10:13:33.82182Z","iopub.execute_input":"2022-01-04T10:13:33.82227Z","iopub.status.idle":"2022-01-04T10:14:29.352306Z","shell.execute_reply.started":"2022-01-04T10:13:33.822235Z","shell.execute_reply":"2022-01-04T10:14:29.35049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In order to make the most of our few training examples, we will \"augment\" them via a number of random transformations, so that our model would never see twice the exact same picture. This helps prevent overfitting and helps the model generalize better.","metadata":{}},{"cell_type":"code","source":"## data augmentation\nfrom keras.preprocessing.image import ImageDataGenerator\n\n## create a generator that rotate, zoom and flip the images\ntraingen = ImageDataGenerator(rotation_range=40,\n        width_shift_range=0.1,\n        height_shift_range=0.1,\n        rescale=1/255,\n        shear_range=0.04,\n        zoom_range=0.2,\n        horizontal_flip=True,\n        vertical_flip= False,\n        fill_mode='nearest')\nvalidgen = ImageDataGenerator(rescale=1/255)\n\n## apply the generator on test and valid sets\ntraingen.fit(train_tensors)\nvalidgen.fit(valid_tensors)\n\ndf_training = traingen.flow(train_tensors , train_targets , batch_size = 20)\ndf_validation = validgen.flow(valid_tensors , valid_targets, batch_size = 20)","metadata":{"id":"jMA7rW0m_09g","execution":{"iopub.status.busy":"2022-01-04T10:14:29.353706Z","iopub.execute_input":"2022-01-04T10:14:29.353937Z","iopub.status.idle":"2022-01-04T10:14:40.799419Z","shell.execute_reply.started":"2022-01-04T10:14:29.35391Z","shell.execute_reply":"2022-01-04T10:14:40.798494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\nmodel.compile(optimizer= Adam(), loss='categorical_crossentropy', metrics=['accuracy'])","metadata":{"id":"t6Rd7IFmACpD","execution":{"iopub.status.busy":"2022-01-04T10:14:40.800783Z","iopub.execute_input":"2022-01-04T10:14:40.802983Z","iopub.status.idle":"2022-01-04T10:14:40.817789Z","shell.execute_reply.started":"2022-01-04T10:14:40.802913Z","shell.execute_reply":"2022-01-04T10:14:40.816959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint\n\ncheckpointer = ModelCheckpoint(filepath='saved_models/weights.initial_scratch_model_aug.hdf5', verbose = 0, save_best_only=True)\nmodel.fit_generator(df_training, epochs = 25 , steps_per_epoch = train_tensors.shape[0]//32 , \n                   callbacks=[checkpointer] , verbose=1 , \n                   validation_data= df_validation , validation_steps = valid_tensors.shape[0]//32)","metadata":{"id":"ae9ncQK9Aehf","execution":{"iopub.status.busy":"2022-01-04T10:14:40.822508Z","iopub.execute_input":"2022-01-04T10:14:40.823215Z","iopub.status.idle":"2022-01-04T10:56:19.598043Z","shell.execute_reply.started":"2022-01-04T10:14:40.823171Z","shell.execute_reply":"2022-01-04T10:56:19.597489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Here, the data augmenation accuracy becomes lower than original data. It usually causes overfitting, but in this case it is underfitting.\n## need to work more on the generator \ndog_breed_predictions_aug = [np.argmax(model.predict(np.expand_dims(tensor, axis = 0))) for tensor in test_tensors]\n\ntest_accuracy_aug = 100*np.sum(np.array(dog_breed_predictions_aug)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions_aug)\nprint('Test accuracy with Data Augmentation: %.f%%' % test_accuracy_aug)","metadata":{"id":"ISNxHGrqAgtn","execution":{"iopub.status.busy":"2022-01-04T10:56:19.598997Z","iopub.execute_input":"2022-01-04T10:56:19.599686Z","iopub.status.idle":"2022-01-04T10:57:15.183244Z","shell.execute_reply.started":"2022-01-04T10:56:19.599644Z","shell.execute_reply":"2022-01-04T10:57:15.182604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_VGG16(tensor): \n\tfrom keras.applications.vgg16 import VGG16, preprocess_input\n\treturn VGG16(weights='imagenet', include_top=False).predict(preprocess_input(tensor))\n\ndef extract_VGG19(tensor):\n\tfrom keras.applications.vgg19 import VGG19, preprocess_input\n\treturn VGG19(weights='imagenet', include_top=False).predict(preprocess_input(tensor))\n\ndef extract_Resnet50(tensor):\n\tfrom keras.applications.resnet50 import ResNet50, preprocess_input\n\treturn ResNet50(weights='imagenet', include_top=False).predict(preprocess_input(tensor))\n\ndef extract_Xception(tensor):\n\tfrom keras.applications.xception import Xception, preprocess_input\n\treturn Xception(weights='imagenet', include_top=False).predict(preprocess_input(tensor))\n\ndef extract_InceptionV3(tensor):\n\tfrom keras.applications.inception_v3 import InceptionV3, preprocess_input\n\treturn InceptionV3(weights='imagenet', include_top=False).predict(preprocess_input(tensor))","metadata":{"execution":{"iopub.status.busy":"2022-01-04T10:57:15.184652Z","iopub.execute_input":"2022-01-04T10:57:15.185437Z","iopub.status.idle":"2022-01-04T10:57:15.194268Z","shell.execute_reply.started":"2022-01-04T10:57:15.18539Z","shell.execute_reply":"2022-01-04T10:57:15.193481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Use a CNN to Classify Dog Breeds**\nTo reduce training time without sacrificing accuracy, train a CNN using transfer learning. ","metadata":{}},{"cell_type":"code","source":"# Obtain Bottleneck Features\nimport numpy as np\nbottleneck_features = np.load(r'../input/vgg16-data/DogVGG16Data.npz')\ntrain_VGG16 = bottleneck_features['train']\nvalid_VGG16 = bottleneck_features['valid']\ntest_VGG16 = bottleneck_features['test']\n\ntrain_VGG16.shape[1:]","metadata":{"id":"N0gUR8IKB8In","execution":{"iopub.status.busy":"2022-01-04T10:57:15.195521Z","iopub.execute_input":"2022-01-04T10:57:15.196201Z","iopub.status.idle":"2022-01-04T10:57:27.903155Z","shell.execute_reply.started":"2022-01-04T10:57:15.19617Z","shell.execute_reply":"2022-01-04T10:57:27.902259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Model Architecture**\nThe model uses the the pre-trained VGG-16 model as a fixed feature extractor, where the last convolutional output of VGG-16 is fed as input to our model. We only add a global average pooling layer and a fully connected layer, where the latter contains one node for each dog category and is equipped with a softmax.","metadata":{"id":"PSx-Vmv6CCpO"}},{"cell_type":"code","source":"VGG16_model = Sequential()\nVGG16_model.add(GlobalAveragePooling2D(input_shape=train_VGG16.shape[1:]))\nVGG16_model.add(Dense(133, activation='softmax'))\n\nVGG16_model.summary()","metadata":{"id":"P8p2EunSCCCd","execution":{"iopub.status.busy":"2022-01-04T10:57:27.90464Z","iopub.execute_input":"2022-01-04T10:57:27.904867Z","iopub.status.idle":"2022-01-04T10:57:27.934027Z","shell.execute_reply.started":"2022-01-04T10:57:27.904839Z","shell.execute_reply":"2022-01-04T10:57:27.933134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Compile the Model**","metadata":{"id":"mRSjvE4GCTQN"}},{"cell_type":"code","source":"VGG16_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])","metadata":{"id":"yuN43deVCOit","execution":{"iopub.status.busy":"2022-01-04T10:57:27.93545Z","iopub.execute_input":"2022-01-04T10:57:27.935674Z","iopub.status.idle":"2022-01-04T10:57:27.945563Z","shell.execute_reply.started":"2022-01-04T10:57:27.935646Z","shell.execute_reply":"2022-01-04T10:57:27.944602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Train the Model**","metadata":{"id":"jeO5dzdlCXnO"}},{"cell_type":"code","source":"checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.VGG16.hdf5', \n                               verbose=1, save_best_only=True)\n\nVGG16_model.fit(train_VGG16, train_targets, \n          validation_data=(valid_VGG16, valid_targets),\n          epochs=20, batch_size=20, callbacks=[checkpointer], verbose=1)","metadata":{"id":"AHD6mi8FCr_F","execution":{"iopub.status.busy":"2022-01-04T10:57:27.947133Z","iopub.execute_input":"2022-01-04T10:57:27.94739Z","iopub.status.idle":"2022-01-04T10:57:51.19193Z","shell.execute_reply.started":"2022-01-04T10:57:27.947358Z","shell.execute_reply":"2022-01-04T10:57:51.190921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the Model with the Best Validation Loss\nVGG16_model.load_weights('saved_models/weights.best.VGG16.hdf5')","metadata":{"id":"RfTdyVYxCzrN","execution":{"iopub.status.busy":"2022-01-04T10:57:51.193079Z","iopub.execute_input":"2022-01-04T10:57:51.19366Z","iopub.status.idle":"2022-01-04T10:57:51.202865Z","shell.execute_reply.started":"2022-01-04T10:57:51.193622Z","shell.execute_reply":"2022-01-04T10:57:51.201841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test the Model\n\n# get index of predicted dog breed for each image in test set\nVGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\n\n# report test accuracy\ntest_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)","metadata":{"id":"GszWUUYSC4JX","execution":{"iopub.status.busy":"2022-01-04T10:57:51.204564Z","iopub.execute_input":"2022-01-04T10:57:51.205024Z","iopub.status.idle":"2022-01-04T10:58:39.480641Z","shell.execute_reply.started":"2022-01-04T10:57:51.204989Z","shell.execute_reply":"2022-01-04T10:58:39.478667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Predict Dog Breed with the Model**","metadata":{"id":"wBgbnUQdDGb7"}},{"cell_type":"code","source":"#from extract_bottleneck_features import extract_VGG16\n\ndef VGG16_predict_breed(img_path):\n    # extract bottleneck features\n    bottleneck_feature = extract_VGG16(path_to_tensor(img_path))\n    # obtain predicted vector\n    predicted_vector = VGG16_model.predict(bottleneck_feature)\n    # return dog breed that is predicted by the model\n    return dog_names[np.argmax(predicted_vector)]","metadata":{"id":"V_ltcr5pDDnk","execution":{"iopub.status.busy":"2022-01-04T10:58:39.481994Z","iopub.execute_input":"2022-01-04T10:58:39.482277Z","iopub.status.idle":"2022-01-04T10:58:39.487658Z","shell.execute_reply.started":"2022-01-04T10:58:39.482246Z","shell.execute_reply":"2022-01-04T10:58:39.486854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_img(img_path):\n    img = cv2.imread(img_path)\n    cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    imgplot = plt.imshow(cv_rgb)\n    return imgplot","metadata":{"id":"HFR3oPi0DLSD","execution":{"iopub.status.busy":"2022-01-04T10:58:39.489126Z","iopub.execute_input":"2022-01-04T10:58:39.489787Z","iopub.status.idle":"2022-01-04T10:58:39.506773Z","shell.execute_reply.started":"2022-01-04T10:58:39.489703Z","shell.execute_reply":"2022-01-04T10:58:39.505505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Creating a CNN to Classify Dog Breeds (using Transfer Learning)**","metadata":{"id":"4mxQpGQqDW4j"}},{"cell_type":"code","source":"def other_bottleneck_features(path):\n    bottleneck_features = np.load(path)\n    train = bottleneck_features['train'] \n    valid = bottleneck_features['valid']\n    test = bottleneck_features['test']\n    return train,valid,test","metadata":{"id":"wyicqcA8DWc2","execution":{"iopub.status.busy":"2022-01-04T10:58:39.509145Z","iopub.execute_input":"2022-01-04T10:58:39.50953Z","iopub.status.idle":"2022-01-04T10:58:39.522165Z","shell.execute_reply.started":"2022-01-04T10:58:39.509483Z","shell.execute_reply":"2022-01-04T10:58:39.520979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Obtain bottleneck features from another pre-trained CNN.\ntrain_Xception , valid_Xception, test_Xception = other_bottleneck_features('../input/xception/DogXceptionData.npz')\ntrain_Resnet50 , valid_Resnet50, test_Resnet50 = other_bottleneck_features('../input/resnet/DogResnet50Data.npz')\ntrain_VGG19 , valid_VGG19, test_VGG19 = other_bottleneck_features('../input/vgg16-data/DogVGG16Data.npz')\ntrain_Inception , valid_Inception, test_Inception = other_bottleneck_features('../input/inception/DogInceptionV3Data.npz')","metadata":{"id":"ooph6KB7F_4s","execution":{"iopub.status.busy":"2022-01-04T10:58:39.524007Z","iopub.execute_input":"2022-01-04T10:58:39.524569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Xception_model = Sequential()\nXception_model.add(GlobalAveragePooling2D(input_shape=(train_Xception.shape[1:])))\nXception_model.add(Dense(133, activation='softmax'))\n\nXception_model.summary()","metadata":{"id":"XdimmgdBKUpT","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Resnet50_model = Sequential()\nResnet50_model.add(GlobalAveragePooling2D(input_shape=(train_Resnet50.shape[1:])))\nResnet50_model.add(Dense(133, activation='softmax'))\n\nResnet50_model.summary()","metadata":{"id":"6WVp-LPsK1nq","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"VGG19_model = Sequential()\nVGG19_model.add(GlobalAveragePooling2D(input_shape=(train_VGG19.shape[1:])))\nVGG19_model.add(Dense(133, activation='softmax'))\n\nVGG19_model.summary()","metadata":{"id":"XvpQrbJrK2rh","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Inception_model = Sequential()\nInception_model.add(GlobalAveragePooling2D(input_shape=(train_Inception.shape[1:])))\nInception_model.add(Dense(133, activation='softmax'))\n\nInception_model.summary()","metadata":{"id":"hn5CPkW3LABg","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Compile the model\nXception_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\nResnet50_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\nVGG19_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\nInception_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])","metadata":{"id":"uAXce5uZLKRb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Train the model.\ncheckpointer_Xception = ModelCheckpoint(filepath='saved_models/weights.best.Xception.hdf5', verbose=1 , save_best_only =True)\n\nXception_history = Xception_model.fit(train_Xception, train_targets,\n                  validation_data = (valid_Xception , valid_targets),\n                  epochs=25, batch_size=20, callbacks=[checkpointer_Xception], verbose=1)","metadata":{"id":"Zx4uxC9JLOxQ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Train the model.\ncheckpointer_Resnet50 = ModelCheckpoint(filepath='saved_models/weights.best.Resnet50.hdf5', verbose=1 , save_best_only =True)\n\nResnet50_model.fit(train_Resnet50, train_targets,\n                  validation_data = (valid_Resnet50 , valid_targets),\n                  epochs=25, batch_size=20, callbacks=[checkpointer_Resnet50], verbose=1)","metadata":{"id":"LQDewPdXLUUw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Train the model.\ncheckpointer_VGG19 = ModelCheckpoint(filepath='saved_models/weights.best.VGG19.hdf5', verbose=1 , save_best_only =True)\n\nVGG19_model.fit(train_VGG19, train_targets,\n                  validation_data = (valid_VGG19 , valid_targets),\n                  epochs=25, batch_size=20, callbacks=[checkpointer_VGG19], verbose=1)","metadata":{"id":"SgksSweLLYuK","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Train the model.\ncheckpointer_Inception = ModelCheckpoint(filepath='saved_models/weights.best.Inception.hdf5', verbose=1 , save_best_only =True)\n\nInception_model.fit(train_Inception, train_targets,\n                  validation_data = (valid_Inception , valid_targets),\n                  epochs=25, batch_size=20, callbacks=[checkpointer_Inception], verbose=1)","metadata":{"id":"XXhiaAMlLd9w","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Load the model weights with the best validation loss.\nXception_model.load_weights('saved_models/weights.best.Xception.hdf5')\nResnet50_model.load_weights('saved_models/weights.best.Resnet50.hdf5')\nVGG19_model.load_weights('saved_models/weights.best.VGG19.hdf5')\nInception_model.load_weights('saved_models/weights.best.Inception.hdf5')","metadata":{"id":"OpwnhEz0Lh93","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Xception_model.load_weights('saved_models/weights.best.Xception.hdf5')","metadata":{"id":"JeseeoSgLlub","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# a function that returns the prediction accuracy on test data\ndef evaluate_model (model, model_name,tensors,targets):\n    predicted = [np.argmax(model.predict(np.expand_dims(feature, axis=0))) for feature in tensors]\n    test_accuracy = 100*np.sum(np.array(predicted)==np.argmax(targets, axis=1))/len(predicted)\n    \n    print (f'{model_name} accuracy on test data is {test_accuracy}%') ","metadata":{"id":"YPzhuf_7Lni3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"###  Calculate classification accuracy on the test dataset.\nevaluate_model(Xception_model, \"Xception\" , test_Xception, test_targets)\nevaluate_model(Resnet50_model,\"Resnet50\", test_Resnet50, test_targets)\nevaluate_model(VGG19_model,\"VGG19\", test_VGG19, test_targets)\nevaluate_model(Inception_model,\"InceptionV3\", test_Inception, test_targets)","metadata":{"id":"dDgL7H84LrLo","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## plot the history of loss and accuracy for train and valid data for the best model, Xception\nloss = Xception_history.history['loss']\nval_loss = Xception_history.history['val_loss']\n\nplt.figure(figsize=(10,8))\nplt.plot(loss,\"--\", linewidth=3 , label=\"train\")\nplt.plot(val_loss, linewidth=3 , label=\"valid\")\n\nplt.legend(['train','test'], loc='upper left')\nplt.grid()\nplt.ylabel('loss')\nplt.xlabel('Epoch')\nplt.title('Xception Model Loss')\nplt.legend(['train','test'], loc='upper left')\nplt.show()","metadata":{"id":"e6LZLlEMLwnC","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Xception model with augmentation and fine tuning\nfrom keras.optimizers import SGD\nfrom keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\nfrom keras.layers.normalization import BatchNormalization\n\nXception_model_aug = Sequential()\nXception_model_aug.add(GlobalAveragePooling2D(input_shape=(train_Xception.shape[1:])))\nXception_model_aug.add(BatchNormalization())\nXception_model_aug.add(Dense(133, activation='softmax'))\n\ntraingen = ImageDataGenerator(rotation_range=40,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        rescale=1/255,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True,\n        fill_mode='nearest')\nvalidgen = ImageDataGenerator(rescale=1/255)\n\ntraingen.fit(train_Xception)\nvalidgen.fit(valid_Xception)\n\ndf_training = traingen.flow(train_Xception , train_targets , batch_size = 20)\ndf_validation = validgen.flow(valid_Xception , valid_targets, batch_size = 20)\n\ncheckpointer = ModelCheckpoint(filepath='saved_models/weights.best.Xception.hdf5', verbose = 0, save_best_only=True)\n\nsgd = SGD(lr= 1e-3 , decay=1e-6, momentum=0.9 , nesterov = True)\n\n# compile \nXception_model_aug.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n\n\nXception_model_aug.fit_generator(df_training, epochs = 25 , steps_per_epoch = train_Xception.shape[0]//20 , \n                   callbacks=[checkpointer] , verbose=1 , \n                   validation_data= df_validation , validation_steps = valid_Xception.shape[0]//20)","metadata":{"id":"BRmAzjxnLyCL","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## here, again, with data augentation the accuracy decreased\nevaluate_model(Xception_model_aug, \"fine_tuned Xception\" , test_Xception, test_targets)","metadata":{"id":"S_7TUQmjL8qf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## finetuned the Exception model by minimizing the cross-entropy loss function using stochastic gradient descent \n## and learning rate of 0.001\n\nXception_model_aug = Sequential()\nXception_model_aug.add(GlobalAveragePooling2D(input_shape=(train_Xception.shape[1:])))\n# Xception_model_aug.add(BatchNormalization())\nXception_model_aug.add(Dense(133, activation='softmax'))\n\n\ncheckpointer = ModelCheckpoint(filepath='saved_models/weights.best.Xception.hdf5', verbose = 0, save_best_only=True)\n\nsgd = SGD(lr= 1e-3 , decay=1e-6, momentum=0.9 , nesterov = True)\n\n# compile \nXception_model_aug.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n\n\nXception_model_aug.fit(train_Xception , train_targets, \n               validation_data = (valid_Xception, valid_targets),\n               shuffle = True,\n               batch_size = 20,\n               epochs = 25,\n               verbose = 1)","metadata":{"id":"C_lEJkI3MAwG","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## with fine tuning the accuracy increased by almost 1.5% \nevaluate_model(Xception_model_aug, \"fine_tuned Xception\" , test_Xception, test_targets)","metadata":{"id":"Y6YTFcoQMHGE","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## I also wanted to fine tune the Resnet50 model to see how the accuracy will change, since I was sure it woul perform well \n## on this kind of animal images. \n\nResnet50_model_fine = Sequential()\nResnet50_model_fine.add(GlobalAveragePooling2D(input_shape=(train_Xception.shape[1:])))\n# Xception_model_aug.add(BatchNormalization())\nResnet50_model_fine.add(Dense(133, activation='softmax'))\n\n\ncheckpointer = ModelCheckpoint(filepath='saved_models/weights.best.Resnet50.hdf5', verbose = 0, save_best_only=True)\n\nsgd = SGD(lr= 1e-3 , decay=1e-6, momentum=0.9 , nesterov = True)\n\n# compile \nResnet50_model_fine.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n\n\nResnet50_model_fine.fit(train_Xception , train_targets, \n               validation_data = (valid_Xception, valid_targets),\n               shuffle = True,\n               batch_size = 20,\n               epochs = 25,\n               verbose = 1)","metadata":{"id":"h1r-dQF5MHwX","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## the same accuracy as Xception \nevaluate_model(Resnet50_model_fine, \"fine_tuned Resnet50\" , test_Xception, test_targets)","metadata":{"id":"n5zwB1pBMMFA","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### a function that takes a path to an image as input\n### and returns the dog breed that is predicted by the model.\ndef Xception_predict_breed (img_path):\n    # extract the bottle neck features\n    bottleneck_feature = extract_Xception(path_to_tensor(img_path)) \n    ## get a vector of predicted values\n    predicted_vector = Xception_model.predict(bottleneck_feature) \n    \n    ## return the breed\n    return dog_names[np.argmax(predicted_vector)]","metadata":{"id":"qv9DRx7BMO2P","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nfrom matplotlib import pyplot as plt\ndef display_img(img_path):\n    img = cv2.imread(img_path)\n    cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    imgplot = plt.imshow(cv_rgb)\n    return imgplot\n\ndef breed_identifier(img_path):\n    display_img(img_path)\n    prediction = Xception_predict_breed(img_path)\n    if dog_detector(img_path) == True:\n        print('picture is a dog')\n        return print (f\"This dog is a {prediction}\\n\")\n    else:\n        return print('Hm we couldn''t identify, try a differnt photo')","metadata":{"id":"_qoQT-G4MWt3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"breed_identifier(r'../input/testing/dogtest.jfif')","metadata":{"id":"TJN8lA6CMgH-","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"breed_identifier(r'../input/testing/dog2test.jfif')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}